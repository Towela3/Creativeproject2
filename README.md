# Creativeproject2
Artist statement: From the start of this project, I knew I wanted to utilize Adobe Firefly, wanting to challenge myself and the platform to see how it would generate the images prompted. From previous trials of using Adobe Firefly, the results have not been completely positive. When I did want to use it for a nostalgic project, last year my prompts were not being translated the way I wanted them to be compared to other platforms like Midjourney. The few times that Firefly worked out in my favor were when I was designing a quick pseudo-business card and generated a background design based on the initial business logo. It was able to pull the colors from the logo design and create an appealing and decent linear design in return.
When starting this project, I knew I wanted to focus on the flaws of AI, and what it could and couldn’t do well. When thinking about AI especially image-generative AI, the thought of it having any flaws or mistakes doesn’t seem to be a problem I thought would be that hard to solve. My thought process was since a wide range of data is fed into the AI programs, this also accounts for different types of races, ethnicities, and genders. However, upon further research, I was able to better understand why AI has a somewhat hard time generating human features and emotions. emotions seem to be an abreact topic to grasp even for some people, it’s something that cannot always be displayed or if displayed does not always look the same on everyone. With human features especially hands, from my understanding fingers are the main issue because they are not always in the same place or orientation, thus rendering it hard to get a concrete understanding of them.
Over time I did start to get discouraged due to the number of inaccurate results I would get and even thought of switching to Midjourney as I thought it would be able to do a better job. I wanted to stick to the initial plan of using Adobe Firefly, I had hope that within the clusters of images, I would eventually get the “perfect” one. I did have very low expectations with Firefly from previous trials. However, one thing that stood out to me when generating people even though I was not being specific at all; was the fact that it tried to give versions with race. With AI programs being trained by humans, bias is always going to play a factor within the outcome, but it was refreshing to see when I typed in “man being silly with his family”, it didn’t just show me, white men, it was inclusive.

https://youtu.be/YTnEkpL47b4

[Uploading Creative project 2.mp4.zip…]()

Description: What started off as a simple fascination with knowing that AI could not always correctly generate human anatomy translated into trying to see what other aspects AI could not generate, especially in Adobe Firefly. I wanted to focus on human anatomy and emotion and try as many attempts as possible to pull the most “accurate” images I could get. At first, I genially thought it wouldn’t be so hard, I would just type in my prompt of a human showing emotion or portraying an activity and Firefly would generate a human doing said activity. I had my screen split on my computer and when looking at the images then, everything looked fine, I thought this was going too well. Upon further review and a closer look at the images, I realized how horrific the downloaded images looked. The texture of the clothes looked odd, what I thought was a watch was not that at all. The reality finally set in that none of the images I had been downloading were accurate. This is where I enlarged my screen and made sure to review each image quite carefully to ensure that I was downloading and choosing the closest to accurate image possible. Where my second realization came, and I saw the horror of many inaccurate hands.


